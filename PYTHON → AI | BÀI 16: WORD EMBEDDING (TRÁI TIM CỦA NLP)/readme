1ï¸âƒ£ VÃ¬ sao cáº§n Embedding?

âŒ Bag of Words / TF-IDF:

KhÃ´ng hiá»ƒu ngá»¯ nghÄ©a

â€œgoodâ€ â‰  â€œexcellentâ€

âœ… Embedding:

Tá»« cÃ³ Ã½ nghÄ©a gáº§n nhau â†’ vector gáº§n nhau

â€œgoodâ€ â‰ˆ â€œexcellentâ€

â€œbadâ€ â‰ˆ â€œterribleâ€

ğŸ‘‰ MÃ¡y hiá»ƒu ngá»¯ nghÄ©a, khÃ´ng chá»‰ Ä‘áº¿m tá»«

2ï¸âƒ£ Ã tÆ°á»Ÿng Embedding (cá»±c quan trá»ng)

VÃ­ dá»¥:

king  â†’ [0.8, 0.2, 0.1]
queen â†’ [0.79, 0.21, 0.1]
apple â†’ [0.1, 0.9, 0.7]


ğŸ‘‰ king gáº§n queen
ğŸ‘‰ king xa apple

3ï¸âƒ£ Embedding Ä‘Æ¡n giáº£n vá»›i Keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Flatten, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

4ï¸âƒ£ Chuáº©n bá»‹ dá»¯ liá»‡u vÄƒn báº£n
texts = [
    "i love this product",
    "this is amazing",
    "i hate this",
    "this is terrible"
]

labels = [1, 1, 0, 0]

5ï¸âƒ£ Tokenize (chá»¯ â†’ sá»‘)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)

sequences = tokenizer.texts_to_sequences(texts)
X = pad_sequences(sequences, maxlen=5)

print(X)


ğŸ‘‰ Má»—i tá»« â†’ 1 sá»‘
ğŸ‘‰ CÃ¢u â†’ chuá»—i sá»‘

6ï¸âƒ£ XÃ¢y model dÃ¹ng Embedding
vocab_size = len(tokenizer.word_index) + 1

model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=8, input_length=5),
    Flatten(),
    Dense(8, activation="relu"),
    Dense(1, activation="sigmoid")
])

7ï¸âƒ£ Compile & Train
model.compile(
    optimizer="adam",
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

model.fit(X, labels, epochs=100, verbose=0)


ğŸ‰ Báº¡n vá»«a train NLP báº±ng Embedding

8ï¸âƒ£ Dá»± Ä‘oÃ¡n cÃ¢u má»›i
new_text = ["i love this"]
seq = tokenizer.texts_to_sequences(new_text)
seq = pad_sequences(seq, maxlen=5)

print(model.predict(seq))
