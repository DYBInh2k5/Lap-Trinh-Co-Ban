1ï¸âƒ£ Váº¥n Ä‘á» cá»§a RNN / LSTM (lÃ½ do Transformer ra Ä‘á»i)

âŒ Há»c tuáº§n tá»± â†’ cháº­m
âŒ CÃ¢u dÃ i â†’ quÃªn thÃ´ng tin Ä‘áº§u
âŒ KhÃ³ song song hÃ³a

â¡ï¸ Transformer giáº£i quyáº¿t táº¥t cáº£

2ï¸âƒ£ Ã tÆ°á»Ÿng cá»‘t lÃµi cá»§a Transformer

ğŸ‘‰ KhÃ´ng Ä‘á»c tá»«ng tá»« má»™t
ğŸ‘‰ NhÃ¬n toÃ n bá»™ cÃ¢u cÃ¹ng lÃºc

ğŸ”‘ Cá»‘t lÃµi = ATTENTION

3ï¸âƒ£ Attention lÃ  gÃ¬? (HIá»‚U LÃ€ BIáº¾T 50% ChatGPT)

VÃ­ dá»¥ cÃ¢u:

â€œCon mÃ¨o ngá»“i trÃªn cÃ¡i bÃ n vÃ¬ NÃ“ má»‡tâ€

ğŸ‘‰ â€œNÃ“â€ liÃªn quan tá»›i con mÃ¨o, khÃ´ng pháº£i bÃ n

â¡ï¸ Attention giÃºp AI:

Biáº¿t tá»« nÃ o liÃªn quan tá»« nÃ o

Hiá»ƒu ngá»¯ cáº£nh

4ï¸âƒ£ Self-Attention (cá»±c ká»³ quan trá»ng)

Má»—i tá»« sáº½:

NhÃ¬n táº¥t cáº£ tá»« khÃ¡c

Tá»± quyáº¿t Ä‘á»‹nh táº­p trung vÃ o tá»« nÃ o

ğŸ“Œ VÃ­ dá»¥:

I   love   this   product
â†‘     â†‘      â†‘        â†‘


ğŸ‘‰ â€œloveâ€ chÃº Ã½ máº¡nh vÃ o â€œproductâ€

5ï¸âƒ£ CÃ´ng thá»©c (Ã½ tÆ°á»Ÿng, khÃ´ng cáº§n nhá»› toÃ¡n)

Self-Attention dÃ¹ng:

Query (Q)

Key (K)

Value (V)

ğŸ‘‰ So sÃ¡nh Q vá»›i K
ğŸ‘‰ Láº¥y trá»ng sá»‘ nhÃ¢n V

â¡ï¸ MÃ¡y biáº¿t cÃ¡i gÃ¬ quan trá»ng

6ï¸âƒ£ Transformer gá»“m gÃ¬?
ğŸ”¹ Encoder

Hiá»ƒu cÃ¢u

Táº¡o biá»ƒu diá»…n ngá»¯ nghÄ©a

ğŸ”¹ Decoder

Sinh cÃ¢u má»›i (chatbot)

ğŸ‘‰ GPT = chá»‰ Decoder

7ï¸âƒ£ Kiáº¿n trÃºc Transformer (tÃ³m gá»n)
Input
 â†“
Embedding
 â†“
Self-Attention
 â†“
Feed Forward
 â†“
(Repeat nhiá»u láº§n)
 â†“
Output


ğŸ‘‰ ChatGPT cÃ³ hÃ ng trÄƒm layer

8ï¸âƒ£ VÃ¬ sao Transformer máº¡nh?

âœ… Song song â†’ ráº¥t nhanh
âœ… Hiá»ƒu ngá»¯ cáº£nh xa
âœ… Scale cá»±c lá»›n
âœ… Há»c ngÃ´n ngá»¯ tá»± nhiÃªn ráº¥t tá»‘t

â¡ï¸ Thá»‘ng trá»‹ AI tá»« 2018 â†’ nay

9ï¸âƒ£ ChatGPT hoáº¡t Ä‘á»™ng tháº¿ nÃ o? (SIÃŠU QUAN TRá»ŒNG)

1ï¸âƒ£ Há»c tá»« hÃ ng tá»· cÃ¢u
2ï¸âƒ£ Dá»± Ä‘oÃ¡n tá»« tiáº¿p theo
3ï¸âƒ£ Láº·p láº¡i â†’ thÃ nh cÃ¢u tráº£ lá»i
4ï¸âƒ£ Fine-tune báº±ng RLHF (pháº£n há»“i con ngÆ°á»i)

ğŸ‘‰ ChatGPT khÃ´ng â€œbiáº¿tâ€, mÃ  dá»± Ä‘oÃ¡n ráº¥t giá»i
